# -*- coding: utf-8 -*-
"""banben2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18dkxkQTC2l-eVfkK8kUEEixOAEI7EW6M
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
import warnings
warnings.filterwarnings("ignore")



# ----------------------------
# 1) Load dataset
# ----------------------------
train_path = "/content/Train.csv"   # for Colab
df = pd.read_csv(train_path)
print("Dataset loaded. Shape:", df.shape)

raw_df = df.copy(deep=True)  # keep raw copy

# ----------------------------
# 2) Create classification target
# ----------------------------
df['Sales_Class'] = pd.qcut(df['Item_Outlet_Sales'], q=3, labels=["Low", "Medium", "High"])
missing_before = raw_df.isnull().sum().sort_values(ascending=False)

# ----------------------------
# 3) Data cleaning
# ----------------------------
cleaned = df.copy(deep=True)
num_cols = cleaned.select_dtypes(include=[np.number]).columns.tolist()
num_cols = [c for c in num_cols if c not in ['Item_Outlet_Sales']]
cat_cols = cleaned.select_dtypes(include=['object']).columns.tolist()

# Interpretation: unrealistic values
if 'Item_Weight' in cleaned.columns:
    print("Unrealistic Item_Weight values (<=0):", (cleaned['Item_Weight'] <= 0).sum())
    cleaned.loc[cleaned['Item_Weight'] <= 0, 'Item_Weight'] = np.nan

if 'Item_Visibility' in cleaned.columns:
    print("Unrealistic Item_Visibility values (<0):", (cleaned['Item_Visibility'] < 0).sum())
    cleaned.loc[cleaned['Item_Visibility'] < 0, 'Item_Visibility'] = np.nan

# Fill NaNs
for c in num_cols:
    if cleaned[c].isnull().sum() > 0:
        cleaned[c].fillna(cleaned[c].median(), inplace=True)

for c in cat_cols:
    if cleaned[c].isnull().sum() > 0:
        cleaned[c].fillna(cleaned[c].mode()[0], inplace=True)

missing_after = cleaned.isnull().sum().sort_values(ascending=False)

# ----------------------------
# 4) Visualization - Before vs After
# ----------------------------
# Missing values
fig, axes = plt.subplots(1, 2, figsize=(14,5))
axes[0].bar(missing_before.index.astype(str), missing_before.values)
axes[0].set_title("Missing Values — BEFORE Cleaning")
axes[0].tick_params(axis='x', rotation=90)
axes[1].bar(missing_after.index.astype(str), missing_after.values, color='orange')
axes[1].set_title("Missing Values — AFTER Cleaning")
axes[1].tick_params(axis='x', rotation=90)
plt.tight_layout(); plt.show()

# Sales distribution
fig, axes = plt.subplots(1, 2, figsize=(14,5))
axes[0].hist(raw_df['Item_Outlet_Sales'].dropna(), bins=30)
axes[0].set_title("Sales Distribution — BEFORE Cleaning")
axes[0].set_xlabel("Item_Outlet_Sales"); axes[0].set_ylabel("Frequency")
axes[1].hist(cleaned['Item_Outlet_Sales'].dropna(), bins=30, color='orange')
axes[1].set_title("Sales Distribution — AFTER Cleaning")
axes[1].set_xlabel("Item_Outlet_Sales"); axes[1].set_ylabel("Frequency")
plt.tight_layout(); plt.show()

# Class balance
plt.figure(figsize=(6,5))
cleaned['Sales_Class'].value_counts().plot(kind='bar', color=['skyblue','orange','green'])
plt.title("Class Balance of Sales_Class (Low/Medium/High)")
plt.xlabel("Class"); plt.ylabel("Count")
plt.show()

# Correlation heatmap
plt.figure(figsize=(10,8))
corr = cleaned[num_cols].corr(method='pearson')
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Pearson Correlation Heatmap (Numeric Features)")
plt.show()

# ----------------------------
# 5) Prepare data
# ----------------------------
drop_cols = [c for c in ['Item_Identifier', 'Outlet_Identifier'] if c in cleaned.columns]
X = cleaned.drop(columns=drop_cols + ['Item_Outlet_Sales'])
y = X.pop('Sales_Class')
X_encoded = pd.get_dummies(X, drop_first=True)

X_train, X_val, y_train, y_val = train_test_split(
    X_encoded, y, test_size=0.2, random_state=42, stratify=y
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)

# ----------------------------
# 6) Default models
# ----------------------------
default_models = {
    "SVM_rbf": SVC(kernel='rbf', probability=True, random_state=42),
    "KNN_k5": KNeighborsClassifier(n_neighbors=5),
    "ANN_100": MLPClassifier(hidden_layer_sizes=(100,), max_iter=400, random_state=42)
}

default_metrics, preds_default = [], {}
for name, model in default_models.items():
    print(f"Training {name} ...")
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_val_scaled)
    preds_default[name] = y_pred
    default_metrics.append({
        "Model": name,
        "Accuracy": accuracy_score(y_val, y_pred),
        "Precision": precision_score(y_val, y_pred, average='weighted', zero_division=0),
        "Recall": recall_score(y_val, y_pred, average='weighted', zero_division=0),
        "F1": f1_score(y_val, y_pred, average='weighted', zero_division=0)
    })
    print(classification_report(y_val, y_pred, zero_division=0))
    print("-"*50)

default_df = pd.DataFrame(default_metrics).set_index("Model")
print("Default models performance:")
display(default_df)

# ----------------------------
# 7) Tuned models
# ----------------------------
tuned_models = {
    "SVM_linear": SVC(kernel='linear', probability=True, random_state=42),
    "SVM_poly": SVC(kernel='poly', degree=3, probability=True, random_state=42),
    "KNN_k3": KNeighborsClassifier(n_neighbors=3),
    "KNN_k7": KNeighborsClassifier(n_neighbors=7),
    "ANN_2layers": MLPClassifier(hidden_layer_sizes=(100,50), max_iter=600, random_state=42),
    "ANN_3layers": MLPClassifier(hidden_layer_sizes=(150,100,50), max_iter=800, random_state=42)
}

tuned_metrics, preds_tuned = [], {}
for name, model in tuned_models.items():


    print(f"Training {name} ...")
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_val_scaled)
    preds_tuned[name] = y_pred
    tuned_metrics.append({
        "Model": name,
        "Accuracy": accuracy_score(y_val, y_pred),
        "Precision": precision_score(y_val, y_pred, average='weighted', zero_division=0),
        "Recall": recall_score(y_val, y_pred, average='weighted', zero_division=0),
        "F1": f1_score(y_val, y_pred, average='weighted', zero_division=0)
    })
    print(classification_report(y_val, y_pred, zero_division=0))
    print("-"*50)

tuned_df = pd.DataFrame(tuned_metrics).set_index("Model")
print("Tuned models performance:")
display(tuned_df)

# ----------------------------
# 8) Combined comparison
# ----------------------------
all_df = pd.concat([default_df, tuned_df])
print("All models comparison:")
display(all_df)

# ----------------------------
# 9) Visualization - comparison chart
# ----------------------------
ax = all_df[['Accuracy','Precision','Recall','F1']].plot(kind='bar', figsize=(12,6))
plt.title("Model Performance Comparison (Default vs Tuned)")
plt.ylabel("Score")
plt.ylim(0,1)
plt.xticks(rotation=45)
plt.legend(loc="lower right")
plt.show()

# ----------------------------
# 10) Save results
# ----------------------------
all_df.to_csv("/content/classification_metrics_all_models.csv")
print("Saved all model metrics to /content/classification_metrics_all_models.csv")